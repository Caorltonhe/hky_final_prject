{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07945f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, Model, Sequential\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1293d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogLeNet(im_height=224, im_width=224, class_num=1000, aux_logits=False):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    # (None, 224, 224, 3)\n",
    "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"SAME\", activation=\"relu\", name=\"conv2d_1\")(input_image)\n",
    "    # (None, 112, 112, 64)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_1\")(x)\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(64, kernel_size=1, activation=\"relu\", name=\"conv2d_2\")(x)\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(192, kernel_size=3, padding=\"SAME\", activation=\"relu\", name=\"conv2d_3\")(x)\n",
    "    # (None, 56, 56, 192)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_2\")(x)\n",
    "\n",
    "    # (None, 28, 28, 192)\n",
    "    x = Inception(64, 96, 128, 16, 32, 32, name=\"inception_3a\")(x)\n",
    "    # (None, 28, 28, 256)\n",
    "    x = Inception(128, 128, 192, 32, 96, 64, name=\"inception_3b\")(x)\n",
    "\n",
    "    # (None, 28, 28, 480)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_3\")(x)\n",
    "    # (None, 14, 14, 480)\n",
    "    x = Inception(192, 96, 208, 16, 48, 64, name=\"inception_4a\")(x)\n",
    "    if aux_logits:\n",
    "        aux1 = InceptionAux(class_num, name=\"aux_1\")(x)\n",
    "\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(160, 112, 224, 24, 64, 64, name=\"inception_4b\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(128, 128, 256, 24, 64, 64, name=\"inception_4c\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(112, 144, 288, 32, 64, 64, name=\"inception_4d\")(x)\n",
    "    if aux_logits:\n",
    "        aux2 = InceptionAux(class_num, name=\"aux_2\")(x)\n",
    "\n",
    "    # (None, 14, 14, 528)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_4e\")(x)\n",
    "    # (None, 14, 14, 532)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_4\")(x)\n",
    "\n",
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_5a\")(x)\n",
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(384, 192, 384, 48, 128, 128, name=\"inception_5b\")(x)\n",
    "    # (None, 7, 7, 1024)\n",
    "    x = layers.AvgPool2D(pool_size=7, strides=1, name=\"avgpool_1\")(x)\n",
    "\n",
    "    # (None, 1, 1, 1024)\n",
    "    x = layers.Flatten(name=\"output_flatten\")(x)\n",
    "    # (None, 1024)\n",
    "    x = layers.Dropout(rate=0.4, name=\"output_dropout\")(x)\n",
    "    x = layers.Dense(class_num, name=\"output_dense\")(x)\n",
    "    # (None, class_num)\n",
    "    aux3 = layers.Softmax(name=\"aux_3\")(x)\n",
    "\n",
    "    if aux_logits:\n",
    "        model = models.Model(inputs=input_image, outputs=[aux1, aux2, aux3])\n",
    "    else:\n",
    "        model = models.Model(inputs=input_image, outputs=aux3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0dd185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(layers.Layer):\n",
    "    def __init__(self, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.branch1 = layers.Conv2D(ch1x1, kernel_size=1, activation=\"relu\")\n",
    "\n",
    "        self.branch2 = Sequential([\n",
    "            layers.Conv2D(ch3x3red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch3x3, kernel_size=3, padding=\"SAME\", activation=\"relu\")])      # output_size= input_size\n",
    "\n",
    "        self.branch3 = Sequential([\n",
    "            layers.Conv2D(ch5x5red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch5x5, kernel_size=5, padding=\"SAME\", activation=\"relu\")])      # output_size= input_size\n",
    "\n",
    "        self.branch4 = Sequential([\n",
    "            layers.MaxPool2D(pool_size=3, strides=1, padding=\"SAME\"),  # caution: default strides==pool_size\n",
    "            layers.Conv2D(pool_proj, kernel_size=1, activation=\"relu\")])                  # output_size= input_size\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        branch1 = self.branch1(inputs)\n",
    "        branch2 = self.branch2(inputs)\n",
    "        branch3 = self.branch3(inputs)\n",
    "        branch4 = self.branch4(inputs)\n",
    "        outputs = layers.concatenate([branch1, branch2, branch3, branch4])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c4643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionAux(layers.Layer):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(InceptionAux, self).__init__(**kwargs)\n",
    "        self.averagePool = layers.AvgPool2D(pool_size=5, strides=3)\n",
    "        self.conv = layers.Conv2D(128, kernel_size=1, activation=\"relu\")\n",
    "\n",
    "        self.fc1 = layers.Dense(1024, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = self.averagePool(inputs)\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        # N x 2048\n",
    "        x = self.fc1(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x num_classes\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259dffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "    # image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
    "    train_dir = \"C:/桌面/college/大四上/project/flower_dataset/flowers-K-youtube/Train\"\n",
    "    validation_dir = \"C:/桌面/college/大四上/project/flower_dataset/flowers-K-youtube/Validate\"\n",
    "    assert os.path.exists(train_dir), \"cannot find {}\".format(train_dir)\n",
    "    assert os.path.exists(validation_dir), \"cannot find {}\".format(validation_dir)\n",
    "\n",
    "    # create direction for saving weights\n",
    "    if not os.path.exists(\"save_weights\"):\n",
    "        os.makedirs(\"save_weights\")\n",
    "\n",
    "    im_height = 224\n",
    "    im_width = 224\n",
    "    batch_size = 32\n",
    "    epochs = 35\n",
    "\n",
    "    train_accuracy_range = []\n",
    "    val_accuracy_range = []\n",
    "    train_loss_range = []\n",
    "    val_loss_range = []\n",
    "\n",
    "    def pre_function(img):\n",
    "        # img = im.open('test.jpg')\n",
    "        # img = np.array(img).astype(np.float32)\n",
    "        img = img / 255.\n",
    "        img = (img - 0.5) * 2.0\n",
    "\n",
    "        return img\n",
    "\n",
    "    # data generator with data augmentation\n",
    "    train_image_generator = ImageDataGenerator(preprocessing_function=pre_function,\n",
    "                                               horizontal_flip=True)\n",
    "    validation_image_generator = ImageDataGenerator(preprocessing_function=pre_function)\n",
    "\n",
    "    train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               shuffle=True,\n",
    "                                                               target_size=(im_height, im_width),\n",
    "                                                               class_mode='categorical')\n",
    "    total_train = train_data_gen.n\n",
    "\n",
    "    # get class dict\n",
    "    class_indices = train_data_gen.class_indices\n",
    "\n",
    "    # transform value and key of dict\n",
    "    inverse_dict = dict((val, key) for key, val in class_indices.items())\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(inverse_dict, indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  shuffle=False,\n",
    "                                                                  target_size=(im_height, im_width),\n",
    "                                                                  class_mode='categorical')\n",
    "    total_val = val_data_gen.n\n",
    "    print(\"using {} images for training, {} images for validation.\".format(total_train,\n",
    "                                                                           total_val))\n",
    "\n",
    "    model = GoogLeNet(im_height=im_height, im_width=im_width, class_num=5, aux_logits=True)\n",
    "    # model.build((batch_size, 224, 224, 3))  # when using subclass model\n",
    "    model.summary()\n",
    "\n",
    "    # using keras low level api for training\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    optimizer =tf.keras.optimizers.RMSprop(lr=0.009, rho=0.9, epsilon=None, decay=0.0)\n",
    "    \n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "    val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            aux1, aux2, output = model(images, training=True)\n",
    "            loss1 = loss_object(labels, aux1)\n",
    "            loss2 = loss_object(labels, aux2)\n",
    "            loss3 = loss_object(labels, output)\n",
    "            loss = loss1 * 0.3 + loss2 * 0.3 + loss3\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, output)\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(images, labels):\n",
    "        _, _, output = model(images, training=False)\n",
    "        loss = loss_object(labels, output)\n",
    "\n",
    "        val_loss(loss)\n",
    "        val_accuracy(labels, output)\n",
    "\n",
    "\n",
    "    best_val_acc = 0.\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_states()  # clear history info\n",
    "        train_accuracy.reset_states()  # clear history info\n",
    "        val_loss.reset_states()  # clear history info\n",
    "        val_accuracy.reset_states()  # clear history info\n",
    "\n",
    "\n",
    "        # train\n",
    "        train_bar = tqdm(range(total_train // batch_size), file=sys.stdout)\n",
    "\n",
    "        for step in train_bar:\n",
    "            images, labels = next(train_data_gen)\n",
    "            train_step(images, labels)\n",
    "            # print train process\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                                 epochs,\n",
    "                                                                                 train_loss.result(),\n",
    "                                                                                 train_accuracy.result())\n",
    "        train_accuracy_range.append(train_accuracy.result())\n",
    "        train_loss_range.append(train_loss.result())\n",
    "\n",
    "        # validate\n",
    "        val_bar = tqdm(range(total_val // batch_size), file=sys.stdout)\n",
    "        for step in val_bar:\n",
    "            val_images, val_labels = next(val_data_gen)\n",
    "            val_step(val_images, val_labels)\n",
    "\n",
    "            # print val process\n",
    "            val_bar.desc = \"valid epoch[{}/{}] loss:{:.3f}, acc:{:.3f}\".format(epoch + 1,\n",
    "                                                                               epochs,\n",
    "                                                                               val_loss.result(),\n",
    "                                                                               val_accuracy.result())\n",
    "\n",
    "        val_accuracy_range.append(val_accuracy.result())\n",
    "        val_loss_range.append(val_loss.result())\n",
    "\n",
    "\n",
    "        # only save best weights\n",
    "        if val_accuracy.result() > best_val_acc:\n",
    "            best_val_acc = val_accuracy.result()\n",
    "            \n",
    "            model.save_weights(\"C:/Users/HKY/Desktop/college/googLeNet/save_weights/myGoogLeNet.ckpt29\") \n",
    "            \n",
    "\n",
    "        # plot\n",
    "    epochs_range = range(epochs)  # creating a sequence of number from 0 to number of epochs\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_accuracy_range, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs_range, val_accuracy_range, label=\"Validation Accuracy\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_loss_range, label=\"Training Loss\")\n",
    "    plt.plot(epochs_range, val_loss_range, label=\"Validation Loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and validation Loss')\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3278 images belonging to 5 classes.\n",
      "Found 466 images belonging to 5 classes.\n",
      "using 3278 images for training, 466 images for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HKY\\miniconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)        (None, 56, 56, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 56, 56, 64)   4160        maxpool_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 56, 192)  110784      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_2 (MaxPooling2D)        (None, 28, 28, 192)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a (Inception)        (None, 28, 28, 256)  163696      maxpool_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b (Inception)        (None, 28, 28, 480)  388736      inception_3a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_3 (MaxPooling2D)        (None, 14, 14, 480)  0           inception_3b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a (Inception)        (None, 14, 14, 512)  376176      maxpool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b (Inception)        (None, 14, 14, 512)  449160      inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c (Inception)        (None, 14, 14, 512)  510104      inception_4b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d (Inception)        (None, 14, 14, 528)  605376      inception_4c[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e (Inception)        (None, 14, 14, 832)  868352      inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_4 (MaxPooling2D)        (None, 7, 7, 832)    0           inception_4e[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a (Inception)        (None, 7, 7, 832)    1043456     maxpool_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b (Inception)        (None, 7, 7, 1024)   1444080     inception_5a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "avgpool_1 (AveragePooling2D)    (None, 1, 1, 1024)   0           inception_5b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_flatten (Flatten)        (None, 1024)         0           avgpool_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_dropout (Dropout)        (None, 1024)         0           output_flatten[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output_dense (Dense)            (None, 5)            5125        output_dropout[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "aux_1 (InceptionAux)            (None, 5)            2168965     inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_2 (InceptionAux)            (None, 5)            2171013     inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_3 (Softmax)                 (None, 5)            0           output_dense[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,318,655\n",
      "Trainable params: 10,318,655\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "train epoch[1/35] loss:2.756, acc:0.251: 100%|███████████████████████████████████████| 102/102 [00:41<00:00,  2.44it/s]\n",
      "valid epoch[1/35] loss:1.354, acc:0.496: 100%|█████████████████████████████████████████| 14/14 [00:02<00:00,  6.62it/s]\n",
      "train epoch[2/35] loss:2.168, acc:0.390: 100%|███████████████████████████████████████| 102/102 [00:31<00:00,  3.21it/s]\n",
      "valid epoch[2/35] loss:1.299, acc:0.459: 100%|█████████████████████████████████████████| 14/14 [00:03<00:00,  3.80it/s]\n",
      "train epoch[3/35] loss:1.944, acc:0.460: 100%|███████████████████████████████████████| 102/102 [00:23<00:00,  4.38it/s]\n",
      "valid epoch[3/35] loss:1.104, acc:0.567: 100%|█████████████████████████████████████████| 14/14 [00:01<00:00, 12.39it/s]\n",
      "train epoch[4/35] loss:1.822, acc:0.515: 100%|███████████████████████████████████████| 102/102 [00:22<00:00,  4.50it/s]\n",
      "valid epoch[4/35] loss:0.979, acc:0.594: 100%|█████████████████████████████████████████| 14/14 [00:01<00:00, 11.58it/s]\n",
      "train epoch[5/35] loss:1.661, acc:0.589: 100%|███████████████████████████████████████| 102/102 [00:22<00:00,  4.48it/s]\n",
      "valid epoch[5/35] loss:0.964, acc:0.643: 100%|█████████████████████████████████████████| 14/14 [00:01<00:00, 12.54it/s]\n",
      "train epoch[6/35] loss:1.521, acc:0.603: 100%|███████████████████████████████████████| 102/102 [00:22<00:00,  4.44it/s]\n",
      "valid epoch[6/35] loss:0.939, acc:0.560: 100%|█████████████████████████████████████████| 14/14 [00:01<00:00, 12.40it/s]\n",
      "train epoch[7/35] loss:1.533, acc:0.627:  17%|██████▋                                 | 17/102 [00:03<00:19,  4.39it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e861fca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1a13e900340>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flower_category = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
    "model = GoogLeNet(class_num=5, aux_logits=False)\n",
    "weights_path = \"C:/Users/HKY/Desktop/college/googLeNet/save_weights/myGoogLeNet.ckpt29\"\n",
    "assert len(glob.glob(weights_path + \"*\")), \"cannot find {}\".format(weights_path)\n",
    "model.load_weights(weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f73342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flower(test_image):\n",
    "    # resize the test_image\n",
    "    test_image = cv2.resize(test_image, (224, 224))\n",
    "    # convert the image to array\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    # test_image = image_utils.img_to_array(test_image)\n",
    "    # expand the dimensions\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    # predict the image\n",
    "    result = model.predict(test_image)\n",
    "    print(result)\n",
    "    # get the index of the max value\n",
    "    predict_class = np.argmax(result)\n",
    "    # return the flower category\n",
    "\n",
    "    # 该图片为每种花的品种的可能性\n",
    "    # resultAll = np.squeeze(result)\n",
    "    # for i in range(len(resultAll)):\n",
    "    #     print(resultAll[i])\n",
    "\n",
    "    return flower_category[predict_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5be45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 1. 0.]]\n",
      "[[1. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=predict_flower, inputs=\"image\", outputs=\"label\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b2c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
